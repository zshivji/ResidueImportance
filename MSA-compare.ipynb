{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafd31d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "#import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "from Bio import SeqIO\n",
    "import h5py\n",
    "\n",
    "# from scipy.stats import chi2_contingency\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "from sklearn.feature_selection import GenericUnivariateSelect, f_regression, SelectKBest\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder,StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from transformers import logging\n",
    "\n",
    "# from transformers import T5Tokenizer, T5EncoderModel\n",
    "import torch\n",
    "\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, T5Tokenizer\n",
    "\n",
    "# from transformers import T5Tokenizer\n",
    "\n",
    "# from transformers import pipeline\n",
    "\n",
    "# pipe = pipeline(\"translation\", model=\"Rostlab/ProstT5\")\n",
    "\n",
    "# tokenizer = T5Tokenizer.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\", do_lower_case=False)\n",
    "\n",
    "# tokenizer = T5Tokenizer.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\")#, use_fast=False)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfba91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# things to study\n",
    "\n",
    "# huggingface transformers\n",
    "https://huggingface.co/learn/llm-course/chapter1/1?fw=pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3907b05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes\n",
    "\n",
    "# first use ProtT5 encoder only, half precision model (for speed, no k-mer/window size )\n",
    "# feed seqs in 1 at a time (batch size 1)\n",
    "\n",
    "# 1. identify test, train data\n",
    "    # get embeddings for whole protein\n",
    "# 2. build models w/ embeddings as input\n",
    "    # \n",
    "# 3. evaluate models\n",
    "\n",
    "# https://github.com/CalvinRusley/peapod/blob/main/README.md\n",
    "# https://github.com/WRiegs/Squidly/blob/main/squidly/squidly.py#L265\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c08f469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definitions\n",
    "\n",
    "## logits = raw/unnormalized predictions generated by model\n",
    "## softmax = converts logits to probabilities that sum to one\n",
    "## for classification tasks, softmax function generates a vector of (normalized) probabilities with one value for each possible class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac39cde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5EncoderModel\n",
    "import torch\n",
    "import re\n",
    "\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('Rostlab/prot_t5_xl_half_uniref50-enc', do_lower_case=False)\n",
    "\n",
    "# Load the model\n",
    "model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_half_uniref50-enc\").to(device)\n",
    "\n",
    "# gc.collect()\n",
    "\n",
    "# # only GPUs support half-precision currently; if you want to run on CPU use full-precision (not recommended, much slower)\n",
    "# if device == torch.device(\"cpu\"):\n",
    "#     model.to(torch.float32)\n",
    "\n",
    "# only GPUs support half-precision currently; if you want to run on CPU use full-precision (not recommended, much slower)\n",
    "model.full() if device=='cpu' else model.half()\n",
    "\n",
    "# prepare your protein sequences as a dict {accession: sequence})\n",
    "nif = list(SeqIO.parse(\"example.fasta\", \"fasta\"))\n",
    "nif_header = [seq_record.id for seq_record in nif]\n",
    "nif_label = [seq_record.description.split(\"|\")[1] for seq_record in nif]\n",
    "nif_sequences = [str(seq_record.seq) for seq_record in nif]\n",
    "\n",
    "# replace all rare/ambiguous amino acids by X and introduce white-space between all amino acids\n",
    "nif_sequences = [\" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence))) for sequence in nif_sequences]\n",
    "\n",
    "nif_dict = dict(zip(nif_header, nif_sequences))\n",
    "\n",
    "# tokenize sequences and pad up to the longest sequence in the batch\n",
    "#ids = tokenizer(sequence_examples, add_special_tokens=True, padding=\"longest\")\n",
    "ids = tokenizer.batch_encode_plus(nif_sequences, add_special_tokens=True, padding=\"longest\",return_tensors='pt')\n",
    "input_ids = torch.tensor(ids['input_ids']).to(device)\n",
    "attention_mask = torch.tensor(ids['attention_mask']).to(device)\n",
    "\n",
    "# generate embeddings\n",
    "with torch.no_grad():\n",
    "    embedding = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "embedding = embedding.last_hidden_state.cpu().numpy()\n",
    "emb_0_per_protein = emb_0.mean(dim=0)\n",
    "\n",
    "\n",
    "# save embeddings as a dict (without special tokens and padding)\n",
    "nif_embeddings = {}\n",
    "for i, seq_id in enumerate(nif_dict.keys()):\n",
    "    seq_len = (attention_mask[i] == 1).sum()\n",
    "    nif_embeddings[seq_id] = embedding[i][:seq_len]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d29a435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dependencies\n",
    "from pathlib import Path\n",
    "from Bio import SeqIO\n",
    "import pandas as pd\n",
    "from transformers import T5Tokenizer, T5EncoderModel\n",
    "import torch\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e157cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up model\n",
    "\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('Rostlab/prot_t5_xl_half_uniref50-enc', do_lower_case=False)\n",
    "\n",
    "# Load the model\n",
    "model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_half_uniref50-enc\").to(device)\n",
    "\n",
    "# gc.collect()\n",
    "\n",
    "# # only GPUs support half-precision currently; if you want to run on CPU use full-precision (not recommended, much slower)\n",
    "# if device == torch.device(\"cpu\"):\n",
    "#     model.to(torch.float32)\n",
    "\n",
    "# only GPUs support half-precision currently; if you want to run on CPU use full-precision (not recommended, much slower)\n",
    "model.full() if device=='cpu' else model.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70df1ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load fasta files\n",
    "\n",
    "def process_sequence(seq):\n",
    "    return \" \".join(list(re.sub(r\"[UZOB]\", \"X\", seq)))\n",
    "\n",
    "def load_fasta_by_class(fasta_dir):\n",
    "    samples = []\n",
    "    class_to_idx = {}\n",
    "\n",
    "    for i, fasta_file in enumerate(sorted(Path(fasta_dir).glob(\"*.fasta\"))):\n",
    "        class_name = fasta_file.stem\n",
    "        class_to_idx[class_name] = i\n",
    "\n",
    "        for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "            samples.append({\n",
    "                \"id\": record.id,\n",
    "                \"sequence\": process_sequence(str(record.seq)),\n",
    "                \"label\": i,\n",
    "                \"class\": class_name\n",
    "            })\n",
    "\n",
    "    return samples, class_to_idx\n",
    "\n",
    "# generagte embeddings\n",
    "def embed_sequence(seq):\n",
    "\n",
    "    ids = tokenizer.batch_encode_plus(nif_sequences, add_special_tokens=True, padding=\"longest\",return_tensors='pt')\n",
    "    input_ids = torch.tensor(ids['input_ids']).to(device)\n",
    "    attention_mask = torch.tensor(ids['attention_mask']).to(device)\n",
    "\n",
    "    # generate embeddings\n",
    "    with torch.no_grad():\n",
    "        embedding = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    embedding = embedding.last_hidden_state.cpu().numpy()\n",
    "\n",
    "    # Remove special tokens\n",
    "    embedding = embedding[1:-1]  # shape: (L, d)\n",
    "\n",
    "    return embedding.cpu()\n",
    "\n",
    "\n",
    "# save embeddings\n",
    "\n",
    "def embed_and_save(samples, out_dir):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    metadata = []\n",
    "\n",
    "    for s in samples:\n",
    "        emb = embed_sequence(s[\"sequence\"])\n",
    "\n",
    "        out_file = Path(out_dir) / f\"{s['id']}.pt\"\n",
    "        torch.save(emb, out_file)\n",
    "\n",
    "        metadata.append({\n",
    "            \"id\": s[\"id\"],\n",
    "            \"label\": s[\"label\"],\n",
    "            \"class\": s[\"class\"],\n",
    "            \"length\": emb.shape[0],\n",
    "            \"embedding_path\": str(out_file)\n",
    "        })\n",
    "\n",
    "    return metadata\n",
    "\n",
    "\n",
    "samples, class_map = load_fasta_by_class(\"data/fasta\")\n",
    "\n",
    "metadata = embed_and_save(\n",
    "    samples,\n",
    "    out_dir=\"data/embeddings/prostt5\"\n",
    ")\n",
    "\n",
    "df = pd.DataFrame(metadata)\n",
    "df.to_csv(\"data/metadata.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062ac2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "nif_Yhat = pred_model(emb.mean(dim=0,keepdims=True))\n",
    "results[\"mem\"][identifier] = torch.max(nif_Yhat, dim=1)[1].detach().cpu().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cdd8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on new seqs\n",
    "\n",
    "input_fasta_file = \"input_sequences.fasta\"\n",
    "\n",
    "# create results dataframe\n",
    "results_df = pd.DataFrame(columns = ['prot_desc', 'position','site_residue', 'probability', 'prediction'])\n",
    "\n",
    "# load model\n",
    "combined_model = load_model(model_path)\n",
    "\n",
    "for seq_record in tqdm(SeqIO.parse(input_fasta_file, \"fasta\")):\n",
    "    prot_id = seq_record.id\n",
    "    sequence = str(seq_record.seq)\n",
    "    \n",
    "    positive_predicted = []\n",
    "    negative_predicted = []\n",
    "    \n",
    "    # extract protT5 for full sequence and store in temporary dataframe \n",
    "    pt5_all = get_protT5_features(sequence)\n",
    "    \n",
    "    # generate embedding features and window for each amino acid in sequence\n",
    "    for index, amino_acid in enumerate(sequence):\n",
    "        \n",
    "        # check if AA is 'K'\n",
    "        if amino_acid in ['K']:\n",
    "            \n",
    "            # we consider site one more than index, as index starts from 0\n",
    "            site = index + 1\n",
    "            \n",
    "            # extract window\n",
    "            window = extract_one_windows_position(sequence, site)\n",
    "            \n",
    "            # extract embedding_encoding\n",
    "            X_test_embedding = get_input_for_embedding(window)\n",
    "            \n",
    "            # get ProtT5 features extracted above\n",
    "            X_test_pt5 = pt5_all[index]\n",
    "            \n",
    "            # prediction results           \n",
    "            y_pred = combined_model.predict([X_test_embedding.reshape(1, win_size), np.array(X_test_pt5.reshape(1,1024))], verbose = 0)[0][0]\n",
    "            \n",
    "            # append results to results_df\n",
    "            results_df.loc[len(results_df)] = [prot_id, site, amino_acid, y_pred, int(y_pred > cutoff_threshold)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f851fdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on test data\n",
    "\n",
    "# load test data\n",
    "test_positive_pt5 = pd.read_csv(\"data/test/features/test_positive_ProtT5-XL-UniRef50.csv\", header = None).iloc[:,2:]\n",
    "test_negative_pt5 = pd.read_csv(\"data/test/features/test_negative_ProtT5-XL-UniRef50.csv\", header = None).iloc[:,2:]\n",
    "\n",
    "# create labels\n",
    "test_positive_labels = np.ones(test_positive_pt5.shape[0])\n",
    "test_negative_labels = np.zeros(test_negative_pt5.shape[0])\n",
    "\n",
    "# stack positive and negative data together\n",
    "X_test_pt5 = np.vstack((test_positive_pt5,test_negative_pt5))\n",
    "y_test = np.concatenate((test_positive_labels, test_negative_labels), axis = 0)\n",
    "\n",
    "# shuffle X and y together\n",
    "# X_train_pt5, y_train_pt5 = shuffle(X_train_pt5, y_train_pt5)\n",
    "# X_test_pt5, y_test_pt5 = shuffle(X_test_pt5, y_test_pt5)\n",
    "\n",
    "# convert sequences to integer encoding, for embedding\n",
    "test_positive_embedding = get_input_for_embedding('data/test/fasta/test_positive_sites.fasta')\n",
    "test_negative_embedding = get_input_for_embedding('data/test/fasta/test_negative_sites.fasta')\n",
    "\n",
    "# stack positive and negative data together\n",
    "X_test_embedding = np.vstack((test_positive_embedding,test_negative_embedding))\n",
    "\n",
    "# load saved model\n",
    "combined_model = load_model(model_path)\n",
    "\n",
    "# predict test data\n",
    "y_pred = combined_model.predict([X_test_embedding,X_test_pt5]).reshape(y_test.shape[0],)\n",
    "y_pred = (y_pred > cutoff_threshold)\n",
    "y_pred = [int(i) for i in y_pred]\n",
    "y_test = np.array(y_test)\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "# calculate performance metrics\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "sn = cm[1][1]/(cm[1][1]+cm[1][0])\n",
    "sp = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "\n",
    "print(\"\\n %s, %s, %s, %s, %s \\n\" %(str(acc), str(mcc), str(sn), str(sp), cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933f67a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize correlations between numerical features and the target variable\n",
    "correlation_matrix = df_train[numerical_cols].corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2d0e70",
   "metadata": {},
   "source": [
    "Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40ca63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "# RF\n",
    "rf_clf = RandomForestClassifier(max_depth=20, max_features='sqrt', min_samples_split=5, n_estimators=10)\n",
    "rf_clf.fit(X_train_pt5, y_train_pt5)\n",
    "\n",
    "# SVM\n",
    "svm_clf = SVC(C = 3, gamma = 0.01, kernel = 'rbf', probability=True)\n",
    "svm_clf.fit(X_train_pt5, y_train_pt5)\n",
    "\n",
    "#XGB\n",
    "xgb_clf = xgb.XGBRegressor(seed = 321, max_depth =  10, learning_rate = 0.01, n_estimators = 100, colsample_bytree = 0.3)\n",
    "xgb_clf.fit(X_train_pt5, y_train_pt5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faf0d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db02e33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "715ec465",
   "metadata": {},
   "source": [
    "Preprocessing\n",
    "1. Import Data \n",
    "2. Subset data (reduce runtime, improve mafft performance)\n",
    "2. Align sequences (mafft)\n",
    "3. Encode categorical values (Optional, AA, gene annotations)\n",
    "4. Collapse AA into functional groups (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef490d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# align sequences (mafft)\n",
    "os.system(f\"cat in1.fasta in2.fasta > in-merged.fasta\")\n",
    "os.system(f\"mafft --auto --quiet in.fasta > out.aln\")\n",
    "\n",
    "# convert to dataframe, add Y (gene annotation)\n",
    "df = pd.read_csv('out.aln', sep=\"\\t\")  # example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8626c7",
   "metadata": {},
   "source": [
    "Profile/Probabilistic Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500cb478",
   "metadata": {},
   "source": [
    "Statistical Methods\n",
    "1. ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3783c356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ANOVA\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.stats.api as sms\n",
    "\n",
    "def OLS_sm(df,\n",
    "           dependant_var='price',\n",
    "           numeric_features=[],\n",
    "           categorical_features=[],\n",
    "           verbose=False,\n",
    "           show_summary=True,\n",
    "           show_plots=True,\n",
    "           target_is_dollar=True):\n",
    "    \"\"\"\n",
    "    ### Uses formula based statsmodels regression for OLS. ###\n",
    "    \n",
    "    Displays a statsmodels.iolib.summary.Summary object containing summary of OLS analysis. \n",
    "    Returns a statsmodels.regression.linear_model.RegressionResultsWrapper which can be used to access other options available.\n",
    "\n",
    "    Parameters:\n",
    "    ===========\n",
    "        df = pandas.DataFrame; no default. \n",
    "                Input dataset to use for OLS.\n",
    "        dependant_var = str; default: 'price'. \n",
    "                Dependent variable.\n",
    "        numeric_features = list; default = []. \n",
    "                Identify numeric features.\n",
    "        categorical_features = list; default = []. \n",
    "                Identify categorical features.\n",
    "        verbose = boolean; default: False. \n",
    "                Shows some formula used and drop information.\n",
    "                    `True` shows information.\n",
    "                    `False` does not show information.\n",
    "        show_summary = boolean; default: False. \n",
    "                Shows summary report.\n",
    "                    `True` shows information.\n",
    "                    `False` does not show information.\n",
    "        show_plots = boolean; default: True. \n",
    "                Shows summary and Homoscedasticity information.\n",
    "                    `True` shows information.\n",
    "                    `False` does not show information.\n",
    "        target_is_dollar = boolean; default: True. \n",
    "                Modify chart axis label.\n",
    "                    `True` shows information.\n",
    "                    `False` does not show information.    \n",
    "    Under-The-Hood:\n",
    "    =============\n",
    "    --{Major Steps}--\n",
    "        \n",
    "        ## Regression\n",
    "        cate = ' + '.join([f'C({x})' for x in categorical_features])\n",
    "        nume = ' + '.join([f'{x}' for x in numeric_features])\n",
    "        formula = f'{dependant_var} ~ {nume} + {cate}'\n",
    "        \n",
    "        ## plots\n",
    "        # plot on the left\n",
    "        sm.qqplot(multiple_regression.resid,\n",
    "                  dist=stats.norm,\n",
    "                  line='45',\n",
    "                  fit=True,\n",
    "                  ax=ax1)\n",
    "        # plot on the right\n",
    "        ax2.scatter(x=multiple_regression.fittedvalues,\n",
    "                    y=multiple_regression.resid,\n",
    "                    s=4,\n",
    "                    color='gold')\n",
    "    \n",
    "    Note:\n",
    "    =====\n",
    "        Make sure that every column in the DataFrame has the correct dtype.\n",
    "        Numeric values stored as str (i.e, object) will make stats model assume that those are categorical variable.\n",
    "        If Erros, check df to see if the passed feature is available in the DataFrame.\n",
    "    \n",
    "    Issues:\n",
    "    =======\n",
    "        - Output control is not clear.\n",
    "    \n",
    "    Changelog:\n",
    "    ==========\n",
    "        - changed `resid`, was using `resid_pearson`.\n",
    "    \n",
    "    -- ver: 1.3 --\n",
    "    \"\"\"\n",
    "    cate = ' + '.join([f'C({x})' for x in categorical_features])\n",
    "    nume = ' + '.join([f'{x}' for x in numeric_features])\n",
    "    if len(cate)==0:\n",
    "        formula = f'{dependant_var} ~ {nume}'\n",
    "    else:\n",
    "        formula = f'{dependant_var} ~ {nume} + {cate}'\n",
    "    print('Formula for the OLS model: ', formula)\n",
    "    # OLS regressor\n",
    "    multiple_regression = smf.ols(formula=formula, data=df).fit()\n",
    "\n",
    "    if verbose:\n",
    "        show_summary = True\n",
    "        show_plots = True\n",
    "\n",
    "    if show_summary:\n",
    "        display(multiple_regression.summary())\n",
    "    if show_plots:\n",
    "        # plotting\n",
    "        # plot 1\n",
    "        fig, (ax1,\n",
    "              ax2) = plt.subplots(ncols=2,\n",
    "                                  figsize=(10, 5),\n",
    "                                  gridspec_kw={'width_ratios': [0.6, 0.4]})\n",
    "        sm.qqplot(multiple_regression.resid,\n",
    "                  dist=stats.norm,\n",
    "                  line='45',\n",
    "                  fit=True,\n",
    "                  ax=ax1)\n",
    "        ax1.set_title('Q-Q Plot', fontdict={\"size\": 15})\n",
    "        # plot 2\n",
    "        # uses The predicted values for the original (unwhitened) design.\n",
    "        ax2.scatter(x=multiple_regression.fittedvalues, \n",
    "                    y=multiple_regression.resid,\n",
    "                    s=4,\n",
    "                    color='gold')\n",
    "        if target_is_dollar:\n",
    "            ax2.yaxis.set_major_formatter(format_number)\n",
    "        ax2.set(xlabel='Predicted', ylabel='Residuals')\n",
    "        ax2.axhline(y=0, c='r', lw=4, ls='--')\n",
    "        ax2.set_title('Predicted VS Residuals', fontdict={\"size\": 15})\n",
    "        plt.suptitle('Visual Check of Residuals for Homoscedasticity',\n",
    "                     ha='center',\n",
    "                     va='bottom',\n",
    "                     fontdict={\"size\": 25})\n",
    "        plt.tight_layout()\n",
    "    if verbose == False and show_summary == False and show_plots == True:\n",
    "        print('r_sq:', round(multiple_regression.rsquared, 4))\n",
    "    return multiple_regression\n",
    "\n",
    "# Univariate ANOVA\n",
    "''' package can handle categorical variables directly, no need to encode'''\n",
    "\n",
    "stat_list = []\n",
    "for idx, column in enumerate(df.columns):\n",
    "    regression_target = 'annot'\n",
    "    # for dealing with categorical variables\n",
    "    temp_df = df.copy()\n",
    "    for column in df.columns:\n",
    "        f = f'{regression_target} ~ C({column})'\n",
    "    model = smf.ols(formula=f, data=temp_df).fit()\n",
    "    temp_dict = {\n",
    "        'name': column,\n",
    "        'r_sq': model.rsquared,\n",
    "        'intercept': model.params[0],\n",
    "        'beta': model.params[1],\n",
    "        'p_val': model.pvalues[1],\n",
    "        'Jarque-Bera': sms.jarque_bera(model.resid)[0] \n",
    "    }\n",
    "    stat_list.append(temp_dict)\n",
    "df_stat = pd.DataFrame(stat_list).set_index('name')\n",
    "\n",
    "# Multivariate ANOVA\n",
    "OLS_sm(df=df,\n",
    "       numeric_features=[],\n",
    "       dependant_var='price',\n",
    "       categorical_features=df.columns.tolist()[:-1],\n",
    "       show_summary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e954f87b",
   "metadata": {},
   "source": [
    "ML-based methods (Wrapper?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "protML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
